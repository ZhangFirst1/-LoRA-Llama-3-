import json
import os
import random
import argparse
from tqdm import tqdm

# é…ç½®éšæœºç§å­ï¼Œä¿è¯å¤ç°æ€§
random.seed(42)

def parse_adgen_content(content_str):
    """
    è§£æ AdGen çš„ content å­—æ®µ
    è¾“å…¥ç¤ºä¾‹: "ç±»å‹#è£¤*ç‰ˆå‹#å®½æ¾*é£æ ¼#æ€§æ„Ÿ*å›¾æ¡ˆ#çº¿æ¡*è£¤å‹#é˜”è…¿è£¤"
    è¾“å‡ºç¤ºä¾‹: "ç±»å‹: è£¤; ç‰ˆå‹: å®½æ¾; é£æ ¼: æ€§æ„Ÿ; å›¾æ¡ˆ: çº¿æ¡; è£¤å‹: é˜”è…¿è£¤"
    """
    if not content_str:
        return ""
    
    # 1. æ›¿æ¢åˆ†éš”ç¬¦
    # AdGen ä½¿ç”¨ '*' åˆ†éš”å±æ€§ï¼Œ'#' åˆ†éš”é”®å€¼
    properties = content_str.split('*')
    parsed_props = []
    
    for prop in properties:
        if '#' in prop:
            try:
                key, value = prop.split('#', 1)
                parsed_props.append(f"{key}: {value}")
            except ValueError:
                continue
                
    return "; ".join(parsed_props)

def format_data(raw_file_path, output_dir, split_ratio=0.99):
    """
    è¯»å–åŸå§‹æ•°æ®ï¼Œæ¸…æ´—ã€æ ¼å¼åŒ–å¹¶åˆ’åˆ†æ•°æ®é›†
    """
    data_list = []
    
    print(f"ğŸ”„ æ­£åœ¨è¯»å–åŸå§‹æ•°æ®: {raw_file_path} ...")
    
    # è¯»å–åŸå§‹æ•°æ® (å‡è®¾åŸå§‹æ•°æ®æ˜¯ json æ ¼å¼ï¼Œæˆ–è€…æ˜¯æ¯è¡Œä¸€ä¸ª json)
    # è¿™é‡Œå…¼å®¹æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡çš„æ ¼å¼ (JSONL)
    with open(raw_file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        
    
    # å®šä¹‰ System Prompt
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå•†æ–‡æ¡ˆç­–åˆ’å¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹å•†å“å±æ€§ï¼Œæ’°å†™ä¸€æ®µå¸å¼•äººçš„è¥é”€æ–‡æ¡ˆã€‚"
    
    # ç»Ÿè®¡å˜é‡
    total_tokens = 0
    max_len = 0
    skipped_count = 0
    
    for line in tqdm(lines, desc="Processing"):
        try:
            item = json.loads(line.strip())
            
            raw_content = item.get('content', '')
            summary = item.get('summary', '')
            
            # --- æ•°æ®æ¸…æ´—é€»è¾‘ ---
            # 1. è¿‡æ»¤æ‰ summary å¤ªçŸ­çš„æ ·æœ¬ (å¯èƒ½æ˜¯è„æ•°æ®)
            if len(summary) < 10:
                skipped_count += 1
                continue
                
            # 2. è§£æ Input
            parsed_input = parse_adgen_content(raw_content)
            
            if not parsed_input:
                skipped_count += 1
                continue

            # 3. ç®€å•çš„æ–‡æœ¬æ¸…æ´— (å»é™¤å¯èƒ½çš„ HTML æ ‡ç­¾æˆ–ä¹±ç )
            summary = summary.replace("&nbsp;", " ").strip()

            # --- æ„å»º Alpaca æ ¼å¼ ---
            entry = {
                "instruction": system_prompt,
                "input": parsed_input,
                "output": summary
            }
            
            # ç®€å•çš„é•¿åº¦ç»Ÿè®¡ (æŒ‰å­—ç¬¦ä¼°ç®—)
            cur_len = len(parsed_input) + len(summary)
            total_tokens += cur_len
            if cur_len > max_len:
                max_len = cur_len
                
            data_list.append(entry)
            
        except json.JSONDecodeError:
            continue

   
    
    # --- æ•°æ®é›†åˆ’åˆ† ---
    random.shuffle(data_list)
    split_idx = int(len(data_list) * split_ratio)
    
    train_data = data_list[:split_idx]
    dev_data = data_list[split_idx:]
    
    # --- ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ ---
    os.makedirs(output_dir, exist_ok=True)
    
    # --- ä¿å­˜æ–‡ä»¶ ---
    train_path = os.path.join(output_dir, "adgen_train.json")
    dev_path = os.path.join(output_dir, "adgen_dev.json")
    
    with open(train_path, 'w', encoding='utf-8') as f:
        json.dump(train_data, f, ensure_ascii=False, indent=2)
        
    with open(dev_path, 'w', encoding='utf-8') as f:
        json.dump(dev_data, f, ensure_ascii=False, indent=2)
        

    
    # ç”Ÿæˆ dataset_info.json (LLaMA-Factory éœ€è¦ï¼Œè™½ç„¶ä½ ç°åœ¨ç”¨è‡ªå®šä¹‰è„šæœ¬ï¼Œä½†ä¿ç•™è¿™ä¸ªæ˜¯ä¸ªå¥½ä¹ æƒ¯)
    dataset_info = {
        "adgen_train": {
            "file_name": "adgen_train.json",
            "columns": {
                "prompt": "instruction",
                "query": "input",
                "response": "output"
            }
        },
        "adgen_dev": {
            "file_name": "adgen_dev.json",
            "columns": {
                "prompt": "instruction",
                "query": "input",
                "response": "output"
            }
        }
    }
    with open(os.path.join(output_dir, "dataset_info.json"), 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AdGen æ•°æ®é¢„å¤„ç†è„šæœ¬")
    parser.add_argument("--raw_file", type=str, default="./data/raw_data.json", help="åŸå§‹ AdGen æ•°æ®æ–‡ä»¶è·¯å¾„ (JSONLæ ¼å¼)")
    parser.add_argument("--output_dir", type=str, default="./data", help="å¤„ç†åæ•°æ®çš„ä¿å­˜ç›®å½•")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥åŸå§‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.raw_file):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°åŸå§‹æ–‡ä»¶ {args.raw_file}")
        print("è¯·ä¸‹è½½ AdGen æ•°æ®é›† (train.json) å¹¶æ”¾ç½®åœ¨ data ç›®å½•ä¸‹ï¼Œæˆ–ä½¿ç”¨ --raw_file æŒ‡å®šè·¯å¾„ã€‚")
    else:
        format_data(args.raw_file, args.output_dir)